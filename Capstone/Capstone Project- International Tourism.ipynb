{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Pipeline / Data Warehouse for International Tourism\n",
    "### Data Engineering Capstone Project\n",
    "### Matthew Prout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Project Summary\n",
    "The purpose of this project is to use skills I have learned throughout this program to combine data sources into a data warehouse that can be used for further analysis.  For my project, I am using the data sets from the Udacity provided project, and adding another data source for additional insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The purpose of this project is to load data into a Redshift analytical database so that can be used to analyze international tourism.  The data warehouse will support queries used to uncover factors that affect international tourism, such as weather (in the US and abroad), the degree to how international cities and states are to drawing tourism, and whether oil prices affect international tourism.  \n",
    "\n",
    "This will be accomplished in two steps.  The first step will be to clean and explore the data using Apache Spark.  The cleaned data sets will be stored in Parquet files in S3.  The second step will be to run an ETL script which loads the Parquet files from S3 into Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here is a description of the data sources that I am using in the project:\n",
    "\n",
    "| Data Set | Source |Notes |\n",
    "| ------ | ------ | ------ |\n",
    "| I94 Immigration Data | [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html) | A data dictionary I94_SAS_Lablels_Descriptions.SAS describes the contents of this data set. The data set is in an SAS format. |\n",
    "| World Temperature Data | [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) | There are several data sets for different geographic areas. I am using the one for states. The data file is a time series .csv file. |\n",
    "| U.S. City Demographic Data | [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) | Contains demogrpahic information for cities such as population, median age, number of each race, population of each sex. The data file is a .csv file. |\n",
    "| Brent Oil Prices | [Kaggle](https://www.kaggle.com/mabusalah/brent-oil-prices) | Lists the price of Brent oil over time. The file is a .csv file. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Imports and Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sql_queries import visit_key, demographics_key, oilprice_key, tempbystate_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "# AWS credentials\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# S3 bucket\n",
    "s3_bucket = config.get('S3', 'S3A_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Spark release to access S3\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**1. Load the I-94 immigration data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "i94_df = spark.read.parquet(\"sas_data\")\n",
    "\n",
    "# Select the columns of interest\n",
    "i94_df = i94_df.select(['cicid', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94visa'])\n",
    "\n",
    "# Rename the columns\n",
    "i94_df = i94_df.withColumnRenamed(\"cicid\",\"id\") \\\n",
    ".withColumnRenamed(\"i94cit\",\"citizen_code\") \\\n",
    ".withColumnRenamed(\"i94res\",\"resident_code\") \\\n",
    ".withColumnRenamed(\"i94port\",\"port\") \\\n",
    ".withColumnRenamed(\"arrdate\",\"arrival_date_days\") \\\n",
    ".withColumnRenamed(\"i94mode\",\"mode\") \\\n",
    ".withColumnRenamed(\"i94addr\",\"destination_state\") \\\n",
    ".withColumnRenamed(\"depdate\",\"departure_date_days\") \\\n",
    ".withColumnRenamed(\"i94visa\",\"visa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- citizen_code: double (nullable = true)\n",
      " |-- resident_code: double (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- arrival_date_days: double (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- destination_state: string (nullable = true)\n",
      " |-- departure_date_days: double (nullable = true)\n",
      " |-- visa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the schema\n",
    "i94_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=5748517.0, citizen_code=245.0, resident_code=438.0, port='LOS', arrival_date_days=20574.0, mode=1.0, destination_state='CA', departure_date_days=20582.0, visa=1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data\n",
    "i94_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Notice that the arrival_date_days and departure_date_days fields are doubles.\n",
    "* These values represent the days since January 1, 1960 and need to be converted to dates\n",
    "\n",
    "Notice that the port is a code.\n",
    "* The port needs to be converted to a port name and state.\n",
    "\n",
    "Notice that citizen_code and resident_code are numbers.\n",
    "* These need to be converted country names.\n",
    "\n",
    "Notice that id, mode, and visa are doubles instead of integers.\n",
    "* Convert these types to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Explore the I-94 immigration data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "i94_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Observation: Over three million rows in the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------+----+-----------------+----+-----------------+-------------------+----+\n",
      "| id|citizen_code|resident_code|port|arrival_date_days|mode|destination_state|departure_date_days|visa|\n",
      "+---+------------+-------------+----+-----------------+----+-----------------+-------------------+----+\n",
      "|  0|           0|            0|   0|                0| 239|           152592|             142457|   0|\n",
      "+---+------------+-------------+----+-----------------+----+-----------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "i94_df.select([count(when(col(c).isNull(), c)).alias(c) for c in i94_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Observation: There are missing 'mode', 'destination_state', and 'departure_date_days' fields.\n",
    "* The rows where 'mode' is null should be removed, as there are so few, but leave the 'destination_state', and 'departure_date_days' fields alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**2. Load the demographic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographicSchema = R([\n",
    "    Fld(\"City\",Str()),\n",
    "    Fld(\"State\",Str()),\n",
    "    Fld(\"MedianAge\",Dbl()),\n",
    "    Fld(\"Male Population\",Int()),\n",
    "    Fld(\"Female Population\",Int()),\n",
    "    Fld(\"Total Population\",Int()),\n",
    "    Fld(\"Number of Veterans\",Int()),\n",
    "    Fld(\"Foreign-born\",Int()),\n",
    "    Fld(\"Average Household Size\",Dbl()),\n",
    "    Fld(\"State Code\",Str()),\n",
    "    Fld(\"Race\",Str()),\n",
    "    Fld(\"Count\",Int())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load and rename the columns\n",
    "demographics_df = spark.read.csv('data/us-cities-demographics.csv', sep=\";\", schema=demographicSchema, header=True)\n",
    "demographics_df = demographics_df.select(col('City').alias('city'),col('State Code').alias('state'),col('MedianAge').alias('median_age'),col('Total Population').alias('total_population'),col('Foreign-born').alias('foreign_born'),col('Race').alias('race'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Look at one city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+----------------+------------+--------------------+\n",
      "|         city|state|median_age|total_population|foreign_born|                race|\n",
      "+-------------+-----+----------+----------------+------------+--------------------+\n",
      "|Silver Spring|   MD|      33.8|           82463|       30908|  Hispanic or Latino|\n",
      "|Silver Spring|   MD|      33.8|           82463|       30908|               White|\n",
      "|Silver Spring|   MD|      33.8|           82463|       30908|Black or African-...|\n",
      "|Silver Spring|   MD|      33.8|           82463|       30908|American Indian a...|\n",
      "|Silver Spring|   MD|      33.8|           82463|       30908|               Asian|\n",
      "+-------------+-----+----------+----------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.where(demographics_df.city == 'Silver Spring').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Notice that there are 5 rows for each city (one for each race).  In this case, I only want one row, and I do not need the 'race' column.\n",
    "* Therefore, filter out the rows by choosing one race.\n",
    "\n",
    "It would be helpful to have a column for the foreign born percent of the population.\n",
    "* This can be created as a calculated column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Explore the demographic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows for one race\n",
    "demographics_df.where(demographics_df.race == 'Hispanic or Latino').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------------+------------+----+\n",
      "|city|state|median_age|total_population|foreign_born|race|\n",
      "+----+-----+----------+----------------+------------+----+\n",
      "|   0|    0|         0|               0|          13|   0|\n",
      "+----+-----+----------+----------------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "demographics_df.select([count(when(col(c).isNull(), c)).alias(c) for c in demographics_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Observation: There are 13 cities where 'foreign_born' is null.\n",
    "* Just remove these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**3. Load the temperature data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperatureSchema = R([\n",
    "    Fld(\"dt\",Date()),\n",
    "    Fld(\"AverageTemperature\",Dbl()),\n",
    "    Fld(\"AverageTemperatureUncertainty\",Dbl()),\n",
    "    Fld(\"State\",Str()),\n",
    "    Fld(\"Country\",Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperatures_df = spark.read.csv('data/GlobalLandTemperaturesByState.csv', schema=temperatureSchema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the columns of interest\n",
    "temperatures_df = temperatures_df.select(['dt', 'AverageTemperature', 'State', 'Country'])\n",
    "\n",
    "# Rename the columns\n",
    "temperatures_df = temperatures_df.withColumnRenamed(\"dt\",\"date\") \\\n",
    ".withColumnRenamed(\"AverageTemperature\",\"average_temperature\") \\\n",
    ".withColumnRenamed(\"State\",\"state\") \\\n",
    ".withColumnRenamed(\"Country\",\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(date=datetime.date(1855, 5, 1), average_temperature=25.544, state='Acre', country='Brazil')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data\n",
    "temperatures_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that for considering US states, 'state' should be converted to a 2 letter abbreviation in order to join with other data sets.\n",
    "* When doing analytical queries in Redshift, be sure to abbreviate US state codes by joining with the StateCodes table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Explore the temperature data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645675"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+-------+\n",
      "|date|average_temperature|state|country|\n",
      "+----+-------------------+-----+-------+\n",
      "|   0|              25648|    0|      0|\n",
      "+----+-------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "temperatures_df.select([count(when(col(c).isNull(), c)).alias(c) for c in temperatures_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are many temperature measurements which are null.  Many of the null values are due to missing measurements for dates in the 18th and 19th centuries.  These early measurements can be ignored and instead use more recent temperature trends which are more complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| min(date)| max(date)|\n",
      "+----------+----------+\n",
      "|1743-11-01|2013-09-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the date range:\n",
    "temperatures_df.agg(F.min('date'), F.max('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the measurements for 2013 are not complete. Get a recent range of dates after the year 2000.\n",
    "* Filter the measurements for dates after 1999 and the last full month in 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**4. Load the oil price data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil_df = spark.read.csv('data/BrentOilPrices.csv', header=True)\n",
    "oil_df = oil_df.withColumnRenamed(\"Date\",\"date\").withColumnRenamed(\"Price\",\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the data has two formats for the date, so create two datasets with different date formats, and then combine.  \n",
    "This will be done here instead of in the cleaning section, as we also want to explore the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Part of the data has the date format as: dd-MMM-yy\n",
    "oil_df1 = oil_df.select(F.to_date(col('date'), 'dd-MMM-yy').alias('date'), col('price'))\n",
    "\n",
    "# The remaining data has the date format as: MMM dd, yyyy\n",
    "oil_df2 = oil_df.select(F.to_date(col('date'), 'MMM dd, yyyy').alias('date'), col('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove rows that are null due to the wrong date format\n",
    "oil_df1 = oil_df1.filter(oil_df1.date.isNotNull())\n",
    "oil_df2 = oil_df2.filter(oil_df2.date.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Combine the datasets\n",
    "oil_df = oil_df1.union(oil_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(date=datetime.date(1987, 5, 20), price='18.63')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data\n",
    "oil_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Explore the oil data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8360"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|date|price|\n",
      "+----+-----+\n",
      "|   0|    0|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "oil_df.select([count(when(col(c).isNull(), c)).alias(c) for c in oil_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Load lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_codes_df = spark.read.csv('data/CountryCodes.csv', header=True).withColumn(\"code\",col(\"code\").cast(IntegerType()))\n",
    "state_codes_df = spark.read.csv('data/StateCodes.csv', header=True)\n",
    "port_codes_df = spark.read.csv('data/PortCodes.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**1. Clean the immigration data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert columns to dates using a user-defined function\n",
    "\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())\n",
    "\n",
    "# Rename columns and drop unused columns\n",
    "i94_df = i94_df.withColumn('arrival_date', udf_datetime_from_sas(\"arrival_date_days\")) \\\n",
    ".withColumn('departure_date', udf_datetime_from_sas(\"departure_date_days\"))\n",
    "\n",
    "i94_df = i94_df.drop('arrival_date_days', 'departure_date_days')\n",
    "\n",
    "# Convert 'mode' and 'visa' from double to integer types\n",
    "i94_df = i94_df.withColumn(\"id\",col(\"id\").cast(IntegerType())).withColumn(\"mode\",col(\"mode\").cast(IntegerType())).withColumn(\"visa\",col(\"visa\").cast(IntegerType()))\n",
    "\n",
    "# Drop the 'mode' rows that are null\n",
    "i94_df = i94_df.filter(i94_df.mode.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5748517, citizen_code=245.0, resident_code=438.0, port='LOS', mode=1, destination_state='CA', visa=1, arrival_date=datetime.date(2016, 4, 30), departure_date=datetime.date(2016, 5, 8))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The following query will perform the following steps:\n",
    "\n",
    "# 1. Convert citizen_code and resident_code to countries\n",
    "# 2. Convert port to port name and port state\n",
    "\n",
    "# Create temporary tables\n",
    "i94_df.createOrReplaceTempView('i94')\n",
    "country_codes_df.createOrReplaceTempView('country_codes')\n",
    "port_codes_df.createOrReplaceTempView('port_codes')\n",
    "\n",
    "# Perform the Spark SQL query\n",
    "i94_df = spark.sql(\"\"\"\n",
    "    SELECT id, citizen, resident, port_codes.port AS port_name, port_codes.state AS port_state, mode, destination_state, visa, arrival_date, departure_date\n",
    "    FROM (\n",
    "        SELECT id, citizen, country_codes.country AS resident, port, mode, destination_state, visa, arrival_date, departure_date\n",
    "        FROM (\n",
    "            SELECT id, country_codes.country AS citizen, resident_code, port, mode, destination_state, visa, arrival_date, departure_date\n",
    "            FROM i94 LEFT JOIN country_codes\n",
    "            ON i94.citizen_code = country_codes.code\n",
    "        ) A\n",
    "        LEFT JOIN country_codes\n",
    "        ON A.resident_code = country_codes.code\n",
    "    ) B\n",
    "    LEFT JOIN port_codes ON B.port = port_codes.code\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep only tourist visas\n",
    "i94_df = i94_df.filter(i94_df.visa == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5748522, citizen='CHINA, PRC', resident='NEW ZEALAND', port_name='HONOLULU', port_state='HI', mode=1, destination_state='HI', visa=2, arrival_date=datetime.date(2016, 4, 30), departure_date=datetime.date(2016, 5, 5)),\n",
       " Row(id=5748523, citizen='CHINA, PRC', resident='NEW ZEALAND', port_name='HONOLULU', port_state='HI', mode=1, destination_state='HI', visa=2, arrival_date=datetime.date(2016, 4, 30), departure_date=datetime.date(2016, 5, 12)),\n",
       " Row(id=5748524, citizen='CHINA, PRC', resident='NEW ZEALAND', port_name='HONOLULU', port_state='HI', mode=1, destination_state='HI', visa=2, arrival_date=datetime.date(2016, 4, 30), departure_date=datetime.date(2016, 5, 12)),\n",
       " Row(id=5748525, citizen='CHINA, PRC', resident='NEW ZEALAND', port_name='HOUSTON', port_state='TX', mode=1, destination_state='FL', visa=2, arrival_date=datetime.date(2016, 4, 30), departure_date=datetime.date(2016, 5, 7)),\n",
       " Row(id=5748526, citizen='CHINA, PRC', resident='NEW ZEALAND', port_name='LOS ANGELES', port_state='CA', mode=1, destination_state='CA', visa=2, arrival_date=datetime.date(2016, 4, 30), departure_date=datetime.date(2016, 5, 7))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df.write.mode('overwrite').parquet(os.path.join(s3_bucket,'i94'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**2. Clean the demographic information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep the demographic information for just one race (the information is duplicated)\n",
    "demographics_df = demographics_df.where(demographics_df.race == 'Hispanic or Latino')\n",
    "\n",
    "# Filter out rows where 'foreign_born' is null\n",
    "demographics_df = demographics_df.filter(demographics_df.foreign_born.isNotNull())\n",
    "\n",
    "# Drop the race column and create a calculated column for 'foreign_born_pct'\n",
    "demographics_df = demographics_df.withColumn('foreign_born_pct', col('foreign_born')/col('total_population')).drop('race', 'foreign_born')\n",
    "\n",
    "# Add the year column (constant as all this data comes from 2015)\n",
    "demographics_df = demographics_df.withColumn('year', F.lit('2015').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- foreign_born_pct: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Silver Spring', state='MD', median_age=33.8, total_population=82463, foreign_born_pct=0.3748105210821823, year=2015),\n",
       " Row(city=\"O'Fallon\", state='MO', median_age=36.0, total_population=85032, foreign_born_pct=0.03844435036221658, year=2015),\n",
       " Row(city='Folsom', state='CA', median_age=40.9, total_population=76368, foreign_born_pct=0.17329247852503665, year=2015),\n",
       " Row(city='Wichita', state='KS', median_age=34.6, total_population=389955, foreign_born_pct=0.10326832583246785, year=2015),\n",
       " Row(city='Sparks', state='NV', median_age=36.1, total_population=96098, foreign_born_pct=0.16327082769672627, year=2015)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_df.write.mode('overwrite').parquet(os.path.join(s3_bucket,'demographics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**3. Clean the temperature information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get temperatures after 2000, and up to the last complete month\n",
    "temperatures_2000_2013_df = temperatures_df.filter( (temperatures_df.date >= F.to_date(F.lit('2000-01-01')).cast(T.DateType())) &\\\n",
    "                                                    (temperatures_df.date < F.to_date(F.lit('2013-09-01')).cast(T.DateType())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+-------+\n",
      "|date|average_temperature|state|country|\n",
      "+----+-------------------+-----+-------+\n",
      "|   0|                  0|    0|      0|\n",
      "+----+-------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "temperatures_2000_2013_df.select([count(when(col(c).isNull(), c)).alias(c) for c in temperatures_2000_2013_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are no null values in the most recent complete range of years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Get the average monthly temperature for each city. This requires aggregating by month for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create year and month fields to aggregate on\n",
    "temperatures_2000_2013_df = temperatures_2000_2013_df.withColumn('year', F.year('date')).withColumn('month', F.month('date'))\n",
    "\n",
    "# Perform the aggregation\n",
    "temperatures_2000_2013_df = temperatures_2000_2013_df.groupBy('year', 'month', 'country', 'state').agg(F.avg('average_temperature').alias('monthly_average_temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2011, month=4, country='Brazil', state='Acre', monthly_average_temp=26.264),\n",
       " Row(year=2000, month=4, country='Russia', state='Adygey', monthly_average_temp=13.386),\n",
       " Row(year=2002, month=11, country='Brazil', state='Alagoas', monthly_average_temp=27.194000000000003),\n",
       " Row(year=2000, month=12, country='Brazil', state='Amazonas', monthly_average_temp=26.776999999999997),\n",
       " Row(year=2000, month=3, country='Russia', state='Amur', monthly_average_temp=-13.56)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_2000_2013_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperatures_2000_2013_df.write.mode('overwrite').parquet(os.path.join(s3_bucket,'temperatures'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**4. Clean the oil information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Get the average monthly oil price. This requires aggregating by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create year and month fields to aggregate on\n",
    "oil_df = oil_df.withColumn('year', F.year('date')).withColumn('month', F.month('date'))\n",
    "\n",
    "# Perform the aggregation\n",
    "oil_df = oil_df.groupBy('year', 'month').agg(F.avg('price').alias('monthly_average_price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=1990, month=7, monthly_average_price=17.16909090909091),\n",
       " Row(year=1997, month=11, monthly_average_price=19.174210526315786),\n",
       " Row(year=1987, month=10, monthly_average_price=18.75772727272728),\n",
       " Row(year=1998, month=2, monthly_average_price=14.0695),\n",
       " Row(year=1995, month=12, monthly_average_price=17.92526315789474)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil_df.write.mode('overwrite').parquet(os.path.join(s3_bucket,'oilprices'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data model is a star schema.  Here is a diagram of the data model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![Schema](./images/Schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The star schema was chosen as it is an efficient design for performing OLAP queries (such as joins and aggregations), which is the purpose of this particular data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Fact Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The fact table `visit` is the I-94 immigration data, which consists of a large number of travel events (in the millions) from international travelers to the United States.  Of interest are what country the visitor is arriving from, where they are arriving, what is their mode of transportation, and when they depart.  The visa field indicates whether the travel was for business or pleasure, and this field will be used to filter out only travelers for pleasure (tourism)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Dimension Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "A `datetime` table is a dimension table for the dates used by the `visit` table.  \n",
    "\n",
    "A `tempbystate` table is a dimension table for the average monthly temperature for states for different countries.  This can be used to explore the correlation between source and destination travel locations with temperature.  \n",
    "\n",
    "`demographics` is a dimension table for the demographics for cities in the United States.  In this case, I am keeping the total population, median age, and the total foreign born percent.  This can be used to explore the relationship between demographics and international tourism.  Note that this information is at the city level, and needs to be rolled up to the state level for the analysis, since visitor information is at the state level.  Also note that the data is for the year 2015.  If this table is used in the analysis, it should only be used in queries where the date is within a few years from 2015.\n",
    "\n",
    "`oilprice` is a dimension table for Brent oil prices in dollars.  This can be used to explore the relationship between oil prices and international travel.  For instance, when oil prices are low, this may influence the cost of travel, which may also influence international travel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Simplified Pipeline for the Capstone Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The following steps are necessary to extract, transform, and load (ETL) the data into the data warehouse:\n",
    "1. Spark is used to load and transform the data into the correct schema.  The data is stored in Parquet format in S3.\n",
    "2. The tables (staging and regular tables) in the data warehouse are created. This should only have to be done once in production.\n",
    "3. The Redshift script to copy parquet to staging tables is executed, followed by the script to insert the data from the staging tables into the fact and dimension tables.\n",
    "5. Finally, data quality checks are run to verify pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here is a diagram of the pipeline for my project:\n",
    "\n",
    "![Simplified Pipeline](images/CapstonePipelineSimple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These preconditions must be true before running the pipeline:\n",
    "* An S3 bucket has been configured to write the Parquet files into.\n",
    "* A Redshift cluster has created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Production Pipeline for the Capstone Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In a production environment several changes are necessary for the pipeline, as the Udacity workspace will not be available:\n",
    "1. The data files will need to reside on S3 or some other server\n",
    "2. Spark will need to run on its own EMR cluster\n",
    "3. The workflow will not be run by a person using a Jupyter notebook, but instead a workflow tool such as Airflow will need to orchestrate the pipeline.\n",
    "\n",
    "Below is a diagram of a pipeline which can be used in a production environment:\n",
    "\n",
    "![Production Pipeline](images/CapstonePipelineProduction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. Run Pipelines to Model the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from create_tables import imp_create_tables\n",
    "\n",
    "# Create the staging and fact/dimension tables in the data warehouse.  Note: This should only be done once in production.\n",
    "imp_create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Load the staging tables, then update the fact and dimension tables with this data.  \n",
    "Note: These steps implement a work-around for the lack of upsert in Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COPY staging_visit\n",
      "FROM 's3://dataengineering-nano-dwh-bucket/i94/'\n",
      "IAM_ROLE 'arn:aws:iam::829000336786:role/dwhRole'\n",
      "FORMAT AS PARQUET;\n",
      "\n",
      "COPY staging_demographics\n",
      "FROM 's3://dataengineering-nano-dwh-bucket/demographics/'\n",
      "IAM_ROLE 'arn:aws:iam::829000336786:role/dwhRole'\n",
      "FORMAT AS PARQUET;\n",
      "\n",
      "COPY staging_oilprice\n",
      "FROM 's3://dataengineering-nano-dwh-bucket/oilprices/'\n",
      "IAM_ROLE 'arn:aws:iam::829000336786:role/dwhRole'\n",
      "FORMAT AS PARQUET;\n",
      "\n",
      "COPY staging_tempbystate\n",
      "FROM 's3://dataengineering-nano-dwh-bucket/temperatures/'\n",
      "IAM_ROLE 'arn:aws:iam::829000336786:role/dwhRole'\n",
      "FORMAT AS PARQUET;\n"
     ]
    }
   ],
   "source": [
    "from etl import imp_update_tables\n",
    "\n",
    "# Load data into the staging tables, and then update the fact and dimension tables from them\n",
    "imp_update_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are several data quality checks added to the pipeline.  \n",
    "First, where possible, NULL constraints have been added to tables in the database.  Primary keys have also been added to the `visit` and `datetime` tables to ensure uniqueness.  \n",
    "Second, after data is loaded into the database, quality checks are also run to verify that the tables in the database are not empty, and also that the latest values from the staging tables were copied over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check on visit table passed\n",
      "Quality check on datetime table passed\n",
      "Quality check on demographics table passed\n",
      "Quality check on oilprice table passed\n",
      "Quality check on tempbystate table passed\n"
     ]
    }
   ],
   "source": [
    "from quality_checks import imp_run_checks\n",
    "\n",
    "# Run data quality checks\n",
    "imp_run_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "**Visit**  \n",
    "Description: This table records visits from international travelers.  \n",
    "Source: US National Tourism and Trade Office  \n",
    "\n",
    "| | |\n",
    "| -- | -- |\n",
    "| id | The unique ID of the visit. |\n",
    "| citizen | The country where the visitor is a citizen. |\n",
    "| resident | The country where the visitor resides. |\n",
    "| port_name | The name of the entry port for the visitor. |\n",
    "| port_state | The state where the entry port is located. |\n",
    "| arrival_date | When the visitor arrived. |\n",
    "| mode | The mode of transportation that the visitor took. 1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not reported' |\n",
    "| destination_state | The destination state of the visitor. |\n",
    "| departure_date | When the visitor departed. |\n",
    "| visa | The type of visa. Types: 1 = Business, 2 = Pleasure, 3 = Student.  This has been filtered for only Pleasure (2). |\n",
    "\n",
    "\n",
    "**DateTime**  \n",
    "Description: This table is a denormalization of the times found in the visit table.  \n",
    "Source: From the Visit table (above)  \n",
    "\n",
    "| | |\n",
    "| -- | -- |\n",
    "| event | The time of the event. |\n",
    "| year | The year of the event. |\n",
    "| month | The month of the event. |\n",
    "| day | The day of the month of the event. |\n",
    "| weekday | Whether the event was a weekday (Mon-Fri). |\n",
    "| weekend | Whether the event was a weekend (Sat-Sun). | \n",
    "\n",
    "**Demographics**  \n",
    "Description: Demographics about the states in the US.  \n",
    "Source: OpenSoft  \n",
    "\n",
    "| | | \n",
    "|--|--|\n",
    "| year | The year that the survey was taken. |\n",
    "| city | The city of for the statistics. |\n",
    "| median_age | The median age of the city. |\n",
    "| total_population | The population of the city. |\n",
    "| foreign_born_pct | The percent of the population that was foreign born. |\n",
    "\n",
    "**TempByState**  \n",
    "Description: A time series of average temperatures for each state in a country. The date range is from Jan 1, 2000 to the last complete month of measurements in 2013.  \n",
    "Source: Kaggle  \n",
    "\n",
    "| | |\n",
    "| -- | -- |\n",
    "| country | The country of the measurement. |\n",
    "| state | The state of the measurement. |\n",
    "| year | The year of the measurement. |\n",
    "| month | The month of the measurement. |\n",
    "| monthly_average_temp | The average temperature for that month. |\n",
    "\n",
    "\n",
    "**OilPrice**  \n",
    "Description: A time series of Brent oil prices.  \n",
    "Source: Kaggle  \n",
    "\n",
    "| | |\n",
    "| -- | -- |\n",
    "| year | The year of the measurement. |\n",
    "| month | The month of the measurement. |\n",
    "| monthly_average_price | The average price of oil for that month. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
